# ç›‘æ§ä¸æ—¥å¿—

æœ¬æ–‡æ¡£è¯´æ˜ AI-NoteBook ç³»ç»Ÿçš„ç›‘æ§å’Œæ—¥å¿—ç®¡ç†æ–¹æ¡ˆï¼Œç¡®ä¿ç”Ÿäº§ç¯å¢ƒçš„ç¨³å®šæ€§å’Œå¯è§‚æµ‹æ€§ã€‚

## ç›‘æ§ä½“ç³»æ¦‚è¿°

### ç›‘æ§å±‚çº§

```mermaid
graph TD
    A[ç”¨æˆ·å±‚ç›‘æ§] --> B[åº”ç”¨å±‚ç›‘æ§]
    B --> C[ä¸­é—´ä»¶å±‚ç›‘æ§]
    C --> D[åŸºç¡€è®¾æ–½å±‚ç›‘æ§]

    A --> A1[å‰ç«¯æ€§èƒ½]
    A --> A2[ç”¨æˆ·ä½“éªŒ]

    B --> B1[APIæ€§èƒ½]
    B --> B2[ä¸šåŠ¡æŒ‡æ ‡]
    B --> B3[é”™è¯¯ç‡]

    C --> C1[æ•°æ®åº“]
    C --> C2[ç¼“å­˜]
    C --> C3[æ¶ˆæ¯é˜Ÿåˆ—]

    D --> D1[CPU/å†…å­˜]
    D --> D2[ç£ç›˜IO]
    D --> D3[ç½‘ç»œ]
```

### ç›‘æ§æŒ‡æ ‡åˆ†ç±»

**1. RED æ–¹æ³•**
- **R**ateï¼ˆé€Ÿç‡ï¼‰ï¼šæ¯ç§’è¯·æ±‚æ•°
- **E**rrorsï¼ˆé”™è¯¯ï¼‰ï¼šé”™è¯¯ç‡
- **D**urationï¼ˆæŒç»­æ—¶é—´ï¼‰ï¼šå“åº”æ—¶é—´

**2. USE æ–¹æ³•**
- **U**tilizationï¼ˆåˆ©ç”¨ç‡ï¼‰ï¼šèµ„æºä½¿ç”¨ç™¾åˆ†æ¯”
- **S**aturationï¼ˆé¥±å’Œåº¦ï¼‰ï¼šèµ„æºæ’é˜Ÿç¨‹åº¦
- **E**rrorsï¼ˆé”™è¯¯ï¼‰ï¼šé”™è¯¯æ•°é‡

## æ—¥å¿—ç³»ç»Ÿ

### æ—¥å¿—ç±»å‹

#### 1. åº”ç”¨æ—¥å¿—

ä½¿ç”¨ Winston ç»“æ„åŒ–æ—¥å¿—ï¼š

```typescript
// logger.config.ts
import * as winston from 'winston'
import * as fs from 'fs'
import * as path from 'path'

// åˆ›å»ºæ—¥å¿—ç›®å½•
const logDir = 'logs'
if (!fs.existsSync(logDir)) {
  fs.mkdirSync(logDir)
}

// è‡ªå®šä¹‰æ—¥å¿—æ ¼å¼
const customFormat = winston.format.combine(
  winston.format.timestamp({ format: 'YYYY-MM-DD HH:mm:ss' }),
  winston.format.errors({ stack: true }),
  winston.format.printf(({ timestamp, level, message, context, trace }) => {
    let log = `${timestamp} [${level.toUpperCase()}]`

    if (context) {
      log += ` [${context}]`
    }

    log += `: ${message}`

    if (trace) {
      log += `\nStack Trace: ${trace}`
    }

    return log
  })
)

// æ—¥å¿—é…ç½®
export const logger = winston.createLogger({
  level: process.env.LOG_LEVEL || 'info',
  format: customFormat,
  transports: [
    // é”™è¯¯æ—¥å¿—
    new winston.transports.File({
      filename: path.join(logDir, 'error.log'),
      level: 'error',
      maxsize: 10 * 1024 * 1024, // 10MB
      maxFiles: 10,
      tailable: true
    }),

    // è­¦å‘Šæ—¥å¿—
    new winston.transports.File({
      filename: path.join(logDir, 'warn.log'),
      level: 'warn',
      maxsize: 10 * 1024 * 1024,
      maxFiles: 5,
      tailable: true
    }),

    // ç»¼åˆæ—¥å¿—
    new winston.transports.File({
      filename: path.join(logDir, 'combined.log'),
      maxsize: 50 * 1024 * 1024, // 50MB
      maxFiles: 20,
      tailable: true
    }),

    // æŒ‰æ—¥æœŸåˆ†å‰²çš„æ—¥å¿—
    new winston.transports.DailyRotateFile({
      filename: path.join(logDir, 'application-%DATE%.log'),
      datePattern: 'YYYY-MM-DD',
      maxSize: '20m',
      maxFiles: '30d'
    })
  ],

  // å¼‚å¸¸å¤„ç†
  exceptionHandlers: [
    new winston.transports.File({
      filename: path.join(logDir, 'exceptions.log')
    })
  ],

  // æœªæ•è·çš„ Promise æ‹’ç»
  rejectionHandlers: [
    new winston.transports.File({
      filename: path.join(logDir, 'rejections.log')
    })
  ]
})

// å¼€å‘ç¯å¢ƒæ·»åŠ æ§åˆ¶å°è¾“å‡º
if (process.env.NODE_ENV !== 'production') {
  logger.add(new winston.transports.Console({
    format: winston.format.combine(
      winston.format.colorize(),
      winston.format.simple()
    )
  }))
}

// æ—¥å¿—çº§åˆ«è¾…åŠ©æ–¹æ³•
export class LoggerService {
  constructor(private context?: string) {}

  debug(message: string, meta?: any) {
    logger.debug(message, { context: this.context, ...meta })
  }

  log(message: string, meta?: any) {
    logger.info(message, { context: this.context, ...meta })
  }

  info(message: string, meta?: any) {
    logger.info(message, { context: this.context, ...meta })
  }

  warn(message: string, meta?: any) {
    logger.warn(message, { context: this.context, ...meta })
  }

  error(message: string, trace?: string, meta?: any) {
    logger.error(message, { context: this.context, trace, ...meta })
  }

  verbose(message: string, meta?: any) {
    logger.verbose(message, { context: this.context, ...meta })
  }
}
```

#### 2. è®¿é—®æ—¥å¿—

Nginx è®¿é—®æ—¥å¿—é…ç½®ï¼š

```nginx
http {
  # è‡ªå®šä¹‰æ—¥å¿—æ ¼å¼
  log_format main '$remote_addr - $remote_user [$time_local] '
                  '"$request" $status $body_bytes_sent '
                  '"$http_referer" "$http_user_agent" '
                  '$request_time $upstream_response_time '
                  '$http_x_forwarded_for';

  log_format detailed '$remote_addr - $remote_user [$time_local] '
                     '"$request" $status $body_bytes_sent '
                     '"$http_referer" "$http_user_agent" '
                     '$request_time $upstream_response_time '
                     '$upstream_addr $upstream_status '
                     '$http_x_forwarded_for';

  # è®¿é—®æ—¥å¿—
  access_log /var/log/nginx/access.log main buffer=32k flush=5s;
  access_log /var/log/nginx/access_detailed.log detailed buffer=32k flush=5s;

  # é”™è¯¯æ—¥å¿—
  error_log /var/log/nginx/error.log warn;
}
```

#### 3. æ•°æ®åº“æ—¥å¿—

PostgreSQL æ—¥å¿—é…ç½®ï¼š

```ini
# /etc/postgresql/15/main/postgresql.conf

# å¯ç”¨æ—¥å¿—
logging_collector = on
log_directory = 'pg_log'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'

# æ—¥å¿—è½®è½¬
log_rotation_age = 1d
log_rotation_size = 100MB

# æ—¥å¿—çº§åˆ«
log_min_duration_statement = 1000  # è®°å½•è¶…è¿‡1ç§’çš„æŸ¥è¯¢
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
log_checkpoints = on
log_connections = on
log_disconnections = on
log_lock_waits = on

# æ…¢æŸ¥è¯¢æ—¥å¿—
log_min_duration_statement = 1000  # 1ç§’
```

#### 4. Redis æ—¥å¿—

```ini
# /etc/redis/redis.conf

# æ—¥å¿—çº§åˆ«: debug, verbose, notice, warning
loglevel notice

# æ—¥å¿—æ–‡ä»¶
logfile /var/log/redis/redis-server.log

# æ…¢æŸ¥è¯¢æ—¥å¿—
slowlog-log-slower-than 10000  # 10ç§’
slowlog-max-len 128
```

### æ—¥å¿—çº§åˆ«å®šä¹‰

| çº§åˆ« | ç”¨é€” | ç¤ºä¾‹åœºæ™¯ |
|------|------|----------|
| **ERROR** | é”™è¯¯äº‹ä»¶ï¼Œéœ€è¦ç«‹å³å…³æ³¨ | API å¤±è´¥ã€æ•°æ®åº“è¿æ¥é”™è¯¯ |
| **WARN** | è­¦å‘Šäº‹ä»¶ï¼Œå¯èƒ½éœ€è¦å…³æ³¨ | é™çº§æœåŠ¡ã€é‡è¯•æ“ä½œ |
| **INFO** | é‡è¦ä¸šåŠ¡äº‹ä»¶ | ç”¨æˆ·ç™»å½•ã€è®¢å•åˆ›å»ºã€ä»»åŠ¡å®Œæˆ |
| **DEBUG** | è°ƒè¯•ä¿¡æ¯ | è¯¦ç»†çš„æ‰§è¡Œæµç¨‹ã€ä¸­é—´å˜é‡ |
| **VERBOSE** | æ›´è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯ | å‡½æ•°å…¥å£/å‡ºå£ã€è¯¦ç»†å‚æ•° |

### æ—¥å¿—æœ€ä½³å®è·µ

#### 1. ç»“æ„åŒ–æ—¥å¿—

```typescript
// âŒ ä¸å¥½çš„åšæ³•
logger.log('User logged in: ' + userId + ' from ' + ip)

// âœ… å¥½çš„åšæ³•
logger.info('User logged in', {
  userId,
  ip,
  userAgent: req.headers['user-agent'],
  timestamp: new Date().toISOString()
})
```

#### 2. å…³é”®æ“ä½œæ—¥å¿—

```typescript
// API è¯·æ±‚æ—¥å¿—
app.use((req, res, next) => {
  const startTime = Date.now()

  res.on('finish', () => {
    const duration = Date.now() - startTime
    logger.info('API Request', {
      method: req.method,
      url: req.url,
      status: res.statusCode,
      duration,
      ip: req.ip,
      userAgent: req.headers['user-agent']
    })
  })

  next()
})

// æ•°æ®åº“æŸ¥è¯¢æ—¥å¿—
prisma.$use(async (params, next) => {
  const before = Date.now()
  const result = await next(params)
  const after = Date.now()

  logger.log(`Query ${params.model}.${params.action}`, {
    duration: after - before,
    params: params.args
  })

  return result
})
```

#### 3. æ•æ„Ÿä¿¡æ¯è¿‡æ»¤

```typescript
// æ•æ„Ÿå­—æ®µè„±æ•
function sanitizeData(data: any): any {
  const sensitiveFields = ['password', 'token', 'secret', 'apiKey']

  if (typeof data !== 'object' || data === null) {
    return data
  }

  const sanitized = { ...data }

  for (const field of sensitiveFields) {
    if (field in sanitized) {
      sanitized[field] = '***REDACTED***'
    }
  }

  return sanitized
}

// ä½¿ç”¨
logger.info('User update', sanitizeData(userData))
```

#### 4. å…³è”è¿½è¸ª ID

```typescript
import { v4 as uuidv4 } from 'uuid'

// ç”Ÿæˆè¯·æ±‚ ID
app.use((req, res, next) => {
  req.id = req.headers['x-request-id'] || uuidv4()
  res.setHeader('X-Request-ID', req.id)
  next()
})

// åœ¨æ—¥å¿—ä¸­ä½¿ç”¨
logger.info('Processing request', {
  requestId: req.id,
  userId: req.user?.id,
  action: 'process_article'
})
```

## æ€§èƒ½ç›‘æ§

### Prometheus + Grafana

#### 1. å®‰è£… Prometheus

```bash
# ä¸‹è½½ Prometheus
wget https://github.com/prometheus/prometheus/releases/download/v2.45.0/prometheus-2.45.0.linux-amd64.tar.gz
tar xvfz prometheus-2.45.0.linux-amd64.tar.gz
cd prometheus-2.45.0.linux-amd64

# é…ç½® prometheus.yml
cat > prometheus.yml <<EOF
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'ainotebook-api'
    static_configs:
      - targets: ['localhost:3000']
    metrics_path: '/metrics'

  - job_name: 'postgres'
    static_configs:
      - targets: ['localhost:9187']

  - job_name: 'redis'
    static_configs:
      - targets: ['localhost:9121']

  - job_name: 'node'
    static_configs:
      - targets: ['localhost:9100']
EOF

# å¯åŠ¨ Prometheus
./prometheus --config.file=prometheus.yml
```

#### 2. åº”ç”¨æŒ‡æ ‡é›†æˆ

```typescript
// metrics.service.ts
import { Counter, Histogram, Gauge, register, collectDefaultMetrics } from 'prom-client'

// æ”¶é›†é»˜è®¤æŒ‡æ ‡ï¼ˆCPUã€å†…å­˜ç­‰ï¼‰
collectDefaultMetrics({ register })

// HTTP è¯·æ±‚è€—æ—¶
export const httpRequestDuration = new Histogram({
  name: 'http_request_duration_seconds',
  help: 'Duration of HTTP requests in seconds',
  labelNames: ['method', 'route', 'status_code', 'user_id'],
  buckets: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2, 5]
})

// HTTP è¯·æ±‚æ€»æ•°
export const httpRequestsTotal = new Counter({
  name: 'http_requests_total',
  help: 'Total number of HTTP requests',
  labelNames: ['method', 'route', 'status_code']
})

// AI è¯·æ±‚è®¡æ•°
export const aiRequestsTotal = new Counter({
  name: 'ai_requests_total',
  help: 'Total number of AI API requests',
  labelNames: ['provider', 'model', 'status']
})

// AI è¯·æ±‚è€—æ—¶
export const aiRequestDuration = new Histogram({
  name: 'ai_request_duration_seconds',
  help: 'Duration of AI API requests in seconds',
  labelNames: ['provider', 'model'],
  buckets: [0.5, 1, 2, 5, 10, 30, 60]
})

// æ•°æ®åº“æŸ¥è¯¢è€—æ—¶
export const dbQueryDuration = new Histogram({
  name: 'db_query_duration_seconds',
  help: 'Duration of database queries in seconds',
  labelNames: ['model', 'operation'],
  buckets: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]
})

// åœ¨çº¿ç”¨æˆ·æ•°
export const onlineUsers = new Gauge({
  name: 'online_users_total',
  help: 'Number of online users'
})

// é˜Ÿåˆ—ä»»åŠ¡æ•°
export const queueJobsPending = new Gauge({
  name: 'queue_jobs_pending',
  help: 'Number of pending jobs in queue',
  labelNames: ['queue_name']
})

// é˜Ÿåˆ—ä»»åŠ¡å¤„ç†è€—æ—¶
export const queueJobDuration = new Histogram({
  name: 'queue_job_duration_seconds',
  help: 'Duration of queue job processing',
  labelNames: ['queue_name', 'job_name', 'status'],
  buckets: [1, 5, 10, 30, 60, 300, 600]
})

// è‡ªå®šä¹‰ä¸šåŠ¡æŒ‡æ ‡
export const articleProcessingTotal = new Counter({
  name: 'article_processing_total',
  help: 'Total number of articles processed',
  labelNames: ['status', 'source_type']
})

// ç¼“å­˜å‘½ä¸­ç‡
export const cacheHits = new Counter({
  name: 'cache_hits_total',
  help: 'Total number of cache hits',
  labelNames: ['cache_type']
})

export const cacheMisses = new Counter({
  name: 'cache_misses_total',
  help: 'Total number of cache misses',
  labelNames: ['cache_type']
})
```

#### 3. ä¸­é—´ä»¶é›†æˆ

```typescript
// metrics.middleware.ts
import { Request, Response, NextFunction } from 'express'
import { httpRequestDuration, httpRequestsTotal } from './metrics.service'

export function metricsMiddleware(req: Request, res: Response, next: NextFunction) {
  const start = Date.now()

  res.on('finish', () => {
    const duration = Date.now() - start
    const route = req.route ? req.route.path : req.path

    // è®°å½•è¯·æ±‚æ€»æ•°
    httpRequestsTotal.inc({
      method: req.method,
      route,
      status_code: res.statusCode
    })

    // è®°å½•è¯·æ±‚è€—æ—¶
    httpRequestDuration.observe({
      method: req.method,
      route,
      status_code: res.statusCode,
      user_id: req.user?.id || 'anonymous'
    }, duration / 1000)
  })

  next()
}
```

#### 4. æš´éœ²æŒ‡æ ‡ç«¯ç‚¹

```typescript
// metrics.controller.ts
import { Controller, Get } from '@nestjs/common'
import { register } from 'prom-client'

@Controller('metrics')
export class MetricsController {
  @Get()
  async getMetrics() {
    return {
      contentType: register.contentType,
      body: await register.metrics()
    }
  }
}

// åœ¨ main.ts ä¸­è®¾ç½®
app.use('/metrics', async (req, res) => {
  res.set('Content-Type', register.contentType)
  res.end(await register.metrics())
})
```

### Grafana ä»ªè¡¨æ¿

#### 1. å®‰è£… Grafana

```bash
# Ubuntu/Debian
sudo wget -q -O - https://packages.grafana.com/gpg.key | sudo apt-key add -
sudo add-apt-repository "deb https://packages.grafana.com/oss/deb stable main"
sudo apt update
sudo apt install grafana

# å¯åŠ¨æœåŠ¡
sudo systemctl start grafana-server
sudo systemctl enable grafana-server

# è®¿é—® http://localhost:3000
# é»˜è®¤ç”¨æˆ·å/å¯†ç : admin/admin
```

#### 2. é…ç½®æ•°æ®æº

åœ¨ Grafana ä¸­æ·»åŠ  Prometheus æ•°æ®æºï¼š

```json
{
  "name": "Prometheus",
  "type": "prometheus",
  "url": "http://localhost:9090",
  "access": "proxy",
  "isDefault": true
}
```

#### 3. å…³é”®ä»ªè¡¨æ¿é¢æ¿

**API æ€§èƒ½ç›‘æ§**

```promql
# è¯·æ±‚é€Ÿç‡ï¼ˆæ¯ç§’è¯·æ±‚æ•°ï¼‰
rate(http_requests_total[5m])

# P95 å“åº”æ—¶é—´
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

# P99 å“åº”æ—¶é—´
histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))

# é”™è¯¯ç‡
rate(http_requests_total{status_code=~"5.."}[5m]) / rate(http_requests_total[5m])
```

**AI æœåŠ¡ç›‘æ§**

```promql
# AI è¯·æ±‚é€Ÿç‡
rate(ai_requests_total[5m])

# AI è¯·æ±‚ P95 è€—æ—¶
histogram_quantile(0.95, rate(ai_request_duration_seconds_bucket[5m]))

# AI é”™è¯¯ç‡
rate(ai_requests_total{status="error"}[5m]) / rate(ai_requests_total[5m])
```

**æ•°æ®åº“ç›‘æ§**

```promql
# æŸ¥è¯¢é€Ÿç‡
rate(db_query_duration_seconds_count[5m])

# æ…¢æŸ¥è¯¢ï¼ˆ>1sï¼‰
rate(db_query_duration_seconds_bucket{le="1.0"}[5m])

# è¿æ¥æ± ä½¿ç”¨ç‡
pg_stat_database_numbackends / pg_settings_max_connections
```

**ç¼“å­˜ç›‘æ§**

```promql
# ç¼“å­˜å‘½ä¸­ç‡
rate(cache_hits_total[5m]) / (rate(cache_hits_total[5m]) + rate(cache_misses_total[5m]))

# Redis å†…å­˜ä½¿ç”¨
redis_memory_used_bytes / redis_memory_max_bytes
```

### åº”ç”¨æ€§èƒ½ç›‘æ§ (APM)

#### ä½¿ç”¨ New Relic

```typescript
// å®‰è£…
npm install newrelic

// åˆå§‹åŒ–
import newrelic from 'newrelic'

// è‡ªåŠ¨æ•è· Express è¯·æ±‚
newrelic.instrumentLoadedModule(
  'express',
  require('express')
)

// è‡ªå®šä¹‰äº‹åŠ¡
app.get('/api/process', (req, res) => {
  const transaction = newrelic.getTransaction()

  // æ·»åŠ è‡ªå®šä¹‰å±æ€§
  transaction.addAttribute('userId', req.user.id)
  transaction.addAttribute('articleId', req.body.articleId)

  // è®°å½•é”™è¯¯
  try {
    // å¤„ç†é€»è¾‘
  } catch (error) {
    newrelic.noticeError(error)
    throw error
  }
})
```

#### ä½¿ç”¨ DataDog

```typescript
npm install dd-trace

import tracer from 'dd-trace'

tracer.init({
  service: 'ainotebook-api',
  env: process.env.NODE_ENV,
  logInjection: true,
  analytics: true
})

// è‡ªåŠ¨è¿½è¸ª Expressã€HTTPã€æ•°æ®åº“
tracer.use('express', {
  middleware: true,
  controller: true
})

tracer.use('pg', {
  service: 'postgresql'
})

tracer.use('redis', {
  service: 'redis'
})
```

## å‘Šè­¦ç³»ç»Ÿ

### Alertmanager é…ç½®

```yaml
# alertmanager.yml
global:
  resolve_timeout: 5m
  slack_api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'

  routes:
    # ä¸¥é‡å‘Šè­¦ -> ç«‹å³é€šçŸ¥
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 0s
      repeat_interval: 5m

    # è­¦å‘Šçº§åˆ« -> Slack
    - match:
        severity: warning
      receiver: 'slack-warnings'

receivers:
  - name: 'default'
    slack_configs:
      - channel: '#alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  - name: 'critical-alerts'
    slack_configs:
      - channel: '#critical-alerts'
        title: 'ğŸš¨ CRITICAL: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        color: 'danger'

    email_configs:
      - to: 'oncall@ainotebook.com'
        subject: 'ğŸš¨ Critical Alert: {{ .GroupLabels.alertname }}'

  - name: 'slack-warnings'
    slack_configs:
      - channel: '#warnings'
        title: 'âš ï¸ WARNING: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        color: 'warning'
```

### å‘Šè­¦è§„åˆ™

```yaml
# alerts.yml
groups:
  - name: api_alerts
    interval: 30s
    rules:
      # é«˜é”™è¯¯ç‡
      - alert: HighErrorRate
        expr: rate(http_requests_total{status_code=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
          service: api
        annotations:
          summary: 'API é”™è¯¯ç‡è¿‡é«˜'
          description: 'API é”™è¯¯ç‡ {{ $value | humanizePercentage }}ï¼ˆè¶…è¿‡5%ï¼‰'

      # é«˜å“åº”æ—¶é—´
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
        for: 10m
        labels:
          severity: warning
          service: api
        annotations:
          summary: 'API å“åº”æ—¶é—´è¿‡é•¿'
          description: 'P95 å“åº”æ—¶é—´ {{ $value }}ç§’ï¼ˆè¶…è¿‡2ç§’ï¼‰'

      # æœåŠ¡ä¸å¯ç”¨
      - alert: ServiceDown
        expr: up{job="ainotebook-api"} == 0
        for: 1m
        labels:
          severity: critical
          service: api
        annotations:
          summary: 'API æœåŠ¡ä¸å¯ç”¨'
          description: 'API æœåŠ¡å·²å®•æœºè¶…è¿‡1åˆ†é’Ÿ'

  - name: ai_service_alerts
    interval: 30s
    rules:
      # AI æœåŠ¡å¤±è´¥ç‡é«˜
      - alert: HighAIFailureRate
        expr: rate(ai_requests_total{status="error"}[5m]) / rate(ai_requests_total[5m]) > 0.1
        for: 5m
        labels:
          severity: critical
          service: ai
        annotations:
          summary: 'AI æœåŠ¡å¤±è´¥ç‡è¿‡é«˜'
          description: 'AI æœåŠ¡å¤±è´¥ç‡ {{ $value | humanizePercentage }}ï¼ˆè¶…è¿‡10%ï¼‰'

      # AI æœåŠ¡å“åº”æ…¢
      - alert: SlowAIResponse
        expr: histogram_quantile(0.95, rate(ai_request_duration_seconds_bucket[5m])) > 30
        for: 10m
        labels:
          severity: warning
          service: ai
        annotations:
          summary: 'AI æœåŠ¡å“åº”ç¼“æ…¢'
          description: 'P95 å“åº”æ—¶é—´ {{ $value }}ç§’ï¼ˆè¶…è¿‡30ç§’ï¼‰'

  - name: database_alerts
    interval: 30s
    rules:
      # æ•°æ®åº“è¿æ¥æ•°è¿‡é«˜
      - alert: HighDatabaseConnections
        expr: pg_stat_database_numbackends / pg_settings_max_connections > 0.8
        for: 5m
        labels:
          severity: warning
          service: database
        annotations:
          summary: 'æ•°æ®åº“è¿æ¥æ•°è¿‡é«˜'
          description: 'æ•°æ®åº“è¿æ¥ä½¿ç”¨ç‡ {{ $value | humanizePercentage }}ï¼ˆè¶…è¿‡80%ï¼‰'

      # æ…¢æŸ¥è¯¢å¢å¤š
      - alert: SlowQueryIncrease
        expr: rate(pg_stat_statement_calls_total{latency_ms > 1000}[5m]) > 10
        for: 5m
        labels:
          severity: warning
          service: database
        annotations:
          summary: 'æ…¢æŸ¥è¯¢æ•°é‡å¢åŠ '
          description: 'æ¯ç§’è¶…è¿‡10ä¸ªæ…¢æŸ¥è¯¢ï¼ˆ>1ç§’ï¼‰'

  - name: system_alerts
    interval: 30s
    rules:
      # CPU ä½¿ç”¨ç‡è¿‡é«˜
      - alert: HighCPUUsage
        expr: rate(process_cpu_seconds_total[5m]) > 0.8
        for: 10m
        labels:
          severity: warning
          service: system
        annotations:
          summary: 'CPU ä½¿ç”¨ç‡è¿‡é«˜'
          description: 'CPU ä½¿ç”¨ç‡ {{ $value | humanizePercentage }}ï¼ˆè¶…è¿‡80%ï¼‰'

      # å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜
      - alert: HighMemoryUsage
        expr: process_resident_memory_bytes / node_memory_MemTotal_bytes > 0.9
        for: 5m
        labels:
          severity: critical
          service: system
        annotations:
          summary: 'å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜'
          description: 'å†…å­˜ä½¿ç”¨ç‡ {{ $value | humanizePercentage }}ï¼ˆè¶…è¿‡90%ï¼‰'

      # ç£ç›˜ç©ºé—´ä¸è¶³
      - alert: LowDiskSpace
        expr: (node_filesystem_avail_bytes / node_filesystem_size_bytes) < 0.1
        for: 5m
        labels:
          severity: critical
          service: system
        annotations:
          summary: 'ç£ç›˜ç©ºé—´ä¸è¶³'
          description: 'ç£ç›˜å‰©ä½™ç©ºé—´ {{ $value | humanizePercentage }}ï¼ˆä½äº10%ï¼‰'
```

### é€šçŸ¥æ¸ é“

#### 1. Slack é›†æˆ

```typescript
// slack.service.ts
import axios from 'axios'

export class SlackAlertService {
  private webhookUrl: string

  constructor() {
    this.webhookUrl = process.env.SLACK_WEBHOOK_URL
  }

  async sendAlert(alert: {
    title: string
    message: string
    severity: 'info' | 'warning' | 'error'
    metadata?: Record<string, any>
  }) {
    const colors = {
      info: '#36a64f',
      warning: '#ff9900',
      error: '#ff0000'
    }

    const attachment = {
      color: colors[alert.severity],
      title: alert.title,
      text: alert.message,
      fields: alert.metadata ? Object.entries(alert.metadata).map(([key, value]) => ({
        title: key,
        value: String(value),
        short: true
      })) : [],
      footer: 'AI-NoteBook Alerts',
      ts: Math.floor(Date.now() / 1000)
    }

    await axios.post(this.webhookUrl, { attachments: [attachment] })
  }
}
```

#### 2. é‚®ä»¶é€šçŸ¥

```typescript
// email.service.ts
import nodemailer from 'nodemailer'

export class EmailAlertService {
  private transporter: nodemailer.Transporter

  constructor() {
    this.transporter = nodemailer.createTransporter({
      host: process.env.SMTP_HOST,
      port: parseInt(process.env.SMTP_PORT),
      secure: true,
      auth: {
        user: process.env.SMTP_USER,
        pass: process.env.SMTP_PASS
      }
    })
  }

  async sendAlert(alert: {
    to: string[]
    subject: string
    text: string
    html?: string
  }) {
    await this.transporter.sendMail({
      from: process.env.SMTP_FROM,
      to: alert.to.join(', '),
      subject: alert.subject,
      text: alert.text,
      html: alert.html || alert.text
    })
  }
}
```

## æ—¥å¿—åˆ†æ

### ELK Stack é›†æˆ

#### 1. Filebeat é…ç½®

```yaml
# filebeat.yml
filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /var/www/ainotebook/backend/logs/*.log
    json.keys_under_root: true
    json.add_error_key: true
    fields:
      service: ainotebook-api
      environment: production

  - type: log
    enabled: true
    paths:
      - /var/log/nginx/access.log
    fields:
      service: nginx
      type: access

  - type: log
    enabled: true
    paths:
      - /var/log/nginx/error.log
    fields:
      service: nginx
      type: error

output.elasticsearch:
  hosts: ["localhost:9200"]
  index: "ainotebook-%{+yyyy.MM.dd}"

setup.kibana:
  host: "localhost:5601"

processors:
  - add_host_metadata:
      when.not.contains.tags: forwarded
  - add_cloud_metadata: ~
```

#### 2. Elasticsearch ç´¢å¼•æ¨¡æ¿

```json
PUT _index_template/ainotebook-logs
{
  "index_patterns": ["ainotebook-*"],
  "template": {
    "settings": {
      "number_of_shards": 1,
      "number_of_replicas": 1,
      "index.lifecycle.name": "logs-policy",
      "index.lifecycle.rollover_alias": "ainotebook"
    },
    "mappings": {
      "properties": {
        "@timestamp": { "type": "date" },
        "level": { "type": "keyword" },
        "message": { "type": "text" },
        "context": { "type": "keyword" },
        "userId": { "type": "keyword" },
        "requestId": { "type": "keyword" },
        "duration": { "type": "long" },
        "status": { "type": "keyword" }
      }
    }
  }
}
```

#### 3. Kibana æŸ¥è¯¢

```json
// æŸ¥æ‰¾é”™è¯¯æ—¥å¿—
{
  "query": {
    "bool": {
      "must": [
        { "match": { "level": "error" } },
        { "range": { "@timestamp": { "gte": "now-1h" } } }
      ]
    }
  }
}

// æŸ¥æ‰¾æ…¢æŸ¥è¯¢
{
  "query": {
    "range": {
      "duration": { "gte": 1000 }
    }
  }
}

// ç»Ÿè®¡é”™è¯¯ç‡
{
  "size": 0,
  "aggs": {
    "error_rate": {
      "terms": {
        "field": "level"
      }
    }
  }
}
```

### æ—¥å¿—æŸ¥è¯¢ç¤ºä¾‹

#### Kibana Query Language (KQL)

```
# æŸ¥æ‰¾ç‰¹å®šç”¨æˆ·çš„æ“ä½œ
userId: "12345"

# æŸ¥æ‰¾é”™è¯¯æ—¥å¿—
level: "error"

# æŸ¥æ‰¾æ…¢ API è¯·æ±‚
duration: > 1000 AND service: "ainotebook-api"

# æŸ¥æ‰¾ç‰¹å®šæ—¶é—´èŒƒå›´çš„é”™è¯¯
level: "error" AND @timestamp: >= "2024-01-20T00:00:00"

# æŸ¥æ‰¾ç‰¹å®šè¯·æ±‚ ID çš„æ‰€æœ‰æ—¥å¿—
requestId: "abc-123-def"

# æŸ¥æ‰¾ç‰¹å®šæ“ä½œçš„æ—¥å¿—
message: "Article processing" AND status: "failed"
```

## ç›‘æ§æœ€ä½³å®è·µ

### 1. ç›‘æ§é‡‘å­—å¡”

```mermaid
graph TD
    A[åº•å±‚åŸºç¡€è®¾æ–½] --> B[ä¸­é—´ä»¶å±‚]
    B --> C[åº”ç”¨å±‚]
    C --> D[ä¸šåŠ¡å±‚]

    A --> A1[CPU/å†…å­˜/ç£ç›˜/ç½‘ç»œ]
    B --> B1[æ•°æ®åº“/ç¼“å­˜/æ¶ˆæ¯é˜Ÿåˆ—]
    C --> C1[API æ€§èƒ½/é”™è¯¯ç‡]
    D --> D1[ç”¨æˆ·æŒ‡æ ‡/ä¸šåŠ¡ KPI]
```

### 2. å‘Šè­¦åˆ†çº§

| çº§åˆ« | å“åº”æ—¶é—´ | ç¤ºä¾‹ | é€šçŸ¥æ–¹å¼ |
|------|----------|------|----------|
| **P0 - Critical** | ç«‹å³ï¼ˆ<5åˆ†é’Ÿï¼‰ | æœåŠ¡å®•æœºã€æ•°æ®ä¸¢å¤± | ç”µè¯ + Slack + Email |
| **P1 - High** | å¿«é€Ÿï¼ˆ<15åˆ†é’Ÿï¼‰ | é”™è¯¯ç‡>10%ã€æ€§èƒ½ä¸¥é‡ä¸‹é™ | Slack + Email |
| **P2 - Medium** | åŠæ—¶ï¼ˆ<1å°æ—¶ï¼‰ | é”™è¯¯ç‡>5%ã€å“åº”å˜æ…¢ | Slack |
| **P3 - Low** | å·¥ä½œæ—¶é—´ | èµ„æºä½¿ç”¨>80%ã€è­¦å‘Š | Email / Ticket |

### 3. ç›‘æ§è¦†ç›–ç‡æ£€æŸ¥æ¸…å•

**åº”ç”¨å±‚é¢**
- [ ] API å“åº”æ—¶é—´ï¼ˆP50/P95/P99ï¼‰
- [ ] API é”™è¯¯ç‡
- [ ] è¯·æ±‚é€Ÿç‡ï¼ˆQPSï¼‰
- [ ] ä¸šåŠ¡æŒ‡æ ‡ï¼ˆæ³¨å†Œæ•°ã€æ´»è·ƒç”¨æˆ·ã€æ–‡ç« å¤„ç†æ•°ï¼‰

**æ•°æ®åº“å±‚é¢**
- [ ] æŸ¥è¯¢æ€§èƒ½
- [ ] è¿æ¥æ± ä½¿ç”¨ç‡
- [ ] æ…¢æŸ¥è¯¢æ•°é‡
- [ ] æ­»é”æ£€æµ‹

**ç¼“å­˜å±‚é¢**
- [ ] ç¼“å­˜å‘½ä¸­ç‡
- [ ] å†…å­˜ä½¿ç”¨ç‡
- [ ] é”®æ•°é‡
- [ ] è¿‡æœŸç­–ç•¥

**ç³»ç»Ÿå±‚é¢**
- [ ] CPU ä½¿ç”¨ç‡
- [ ] å†…å­˜ä½¿ç”¨ç‡
- [ ] ç£ç›˜ I/O å’Œç©ºé—´
- [ ] ç½‘ç»œæµé‡

**AI æœåŠ¡å±‚é¢**
- [ ] API è°ƒç”¨æˆåŠŸç‡
- [ ] å“åº”æ—¶é—´
- [ ] Token ä½¿ç”¨é‡
- [ ] æˆæœ¬ç›‘æ§

### 4. å‘Šè­¦ç–²åŠ³é¢„é˜²

```typescript
// å‘Šè­¦èšåˆå’Œå»é‡
class AlertManager {
  private alertBuffer = new Map<string, Alert>()
  private cooldownPeriod = 5 * 60 * 1000 // 5åˆ†é’Ÿå†·å´æœŸ

  shouldAlert(alert: Alert): boolean {
    const key = this.getAlertKey(alert)
    const lastAlert = this.alertBuffer.get(key)

    if (!lastAlert) {
      this.alertBuffer.set(key, alert)
      return true
    }

    const timeSinceLastAlert = Date.now() - lastAlert.timestamp

    // é¦–æ¬¡å‘Šè­¦æˆ–å†·å´æœŸè¿‡å
    if (timeSinceLastAlert > this.cooldownPeriod) {
      this.alertBuffer.set(key, alert)
      return true
    }

    // æŒ‡æ ‡æ¶åŒ–ï¼Œç«‹å³å‘Šè­¦
    if (alert.severity > lastAlert.severity) {
      this.alertBuffer.set(key, alert)
      return true
    }

    return false
  }

  private getAlertKey(alert: Alert): string {
    return `${alert.type}:${alert.service}:${alert.instance}`
  }
}
```

### 5. ç›‘æ§æ•°æ®ä¿ç•™ç­–ç•¥

```yaml
# Prometheus æ•°æ®ä¿ç•™
./prometheus \
  --storage.tsdb.retention.time=15d \
  --storage.tsdb.retention.size=50GB

# Elasticsearch ç´¢å¼•ç”Ÿå‘½å‘¨æœŸ
PUT _ilm/policy/logs-policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_size": "50GB",
            "max_age": "1d"
          }
        }
      },
      "warm": {
        "min_age": "7d",
        "actions": {
          "shrink": {
            "number_of_shards": 1
          }
        }
      },
      "delete": {
        "min_age": "30d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```

## æ•…éšœæ’æŸ¥æŒ‡å—

### å¸¸è§ç›‘æ§åœºæ™¯

#### 1. API å“åº”æ…¢

```bash
# æŸ¥çœ‹ Prometheus æŒ‡æ ‡
curl http://localhost:9090/api/v1/query?query=http_request_duration_seconds

# æ£€æŸ¥åº”ç”¨æ—¥å¿—
tail -f logs/combined.log | grep "duration"

# æŸ¥çœ‹æ…¢æŸ¥è¯¢
tail -f logs/combined.log | grep "slow"

# æ£€æŸ¥æ•°æ®åº“æ€§èƒ½
psql -c "SELECT * FROM pg_stat_statements ORDER BY total_time DESC LIMIT 10"
```

#### 2. é”™è¯¯ç‡çªå¢

```bash
# æŸ¥çœ‹é”™è¯¯æ—¥å¿—
tail -100 logs/error.log

# ç»Ÿè®¡é”™è¯¯ç±»å‹
grep "ERROR" logs/combined.log | awk '{print $3}' | sort | uniq -c | sort -rn

# æ£€æŸ¥å¤–éƒ¨æœåŠ¡
curl -w "@curl-format.txt" -o /dev/null -s "https://api.volcengine.com/health"

# æŸ¥çœ‹å‘Šè­¦å†å²
curl http://localhost:9093/api/v1/alerts
```

#### 3. å†…å­˜æ³„æ¼

```bash
# ç›‘æ§å†…å­˜ä½¿ç”¨
watch -n 5 'ps aux | grep node | grep ainotebook'

# ç”Ÿæˆ heap snapshot
kill -USR2 <pid>

# åˆ†æ heap snapshot
node --heap-prof /path/to/heap-profile.heapsnapshot
```

### ç›‘æ§é¢æ¿æ¨è

#### 1. Node.js Exporter Dashboard

```json
{
  "dashboard": {
    "title": "Node.js åº”ç”¨ç›‘æ§",
    "panels": [
      {
        "title": "CPU ä½¿ç”¨ç‡",
        "targets": [
          {
            "expr": "rate(process_cpu_seconds_total{job='ainotebook-api'}[5m])"
          }
        ]
      },
      {
        "title": "å†…å­˜ä½¿ç”¨",
        "targets": [
          {
            "expr": "process_resident_memory_bytes{job='ainotebook-api'}"
          }
        ]
      },
      {
        "title": "äº‹ä»¶å¾ªç¯å»¶è¿Ÿ",
        "targets": [
          {
            "expr": "nodejs_eventloop_lag_seconds"
          }
        ]
      }
    ]
  }
}
```

#### 2. PostgreSQL Dashboard

```json
{
  "dashboard": {
    "title": "PostgreSQL ç›‘æ§",
    "panels": [
      {
        "title": "è¿æ¥æ•°",
        "targets": [
          {
            "expr": "pg_stat_database_numbackends"
          }
        ]
      },
      {
        "title": "æŸ¥è¯¢æ€§èƒ½",
        "targets": [
          {
            "expr": "rate(pg_stat_statements_total_time_seconds[5m])"
          }
        ]
      },
      {
        "title": "ç¼“å­˜å‘½ä¸­ç‡",
        "targets": [
          {
            "expr": "pg_stat_database_blks_hit / (pg_stat_database_blks_hit + pg_stat_database_blks_read)"
          }
        ]
      }
    ]
  }
}
```

## ç›¸å…³æ–‡æ¡£

- [éƒ¨ç½²æŒ‡å—](/guide/deployment) - ç³»ç»Ÿéƒ¨ç½²å’Œé…ç½®
- [æ•°æ®åº“æ–‡æ¡£](/guide/database) - æ•°æ®åº“ä¼˜åŒ–å’Œé…ç½®
- [æ¶æ„æ–‡æ¡£](/guide/architecture) - ç³»ç»Ÿæ¶æ„è®¾è®¡
